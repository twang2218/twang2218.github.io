
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>Hadoop 1.2.1 单节点伪分布配置 | Tao Wang&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="Tao Wang">
    
    <meta name="description" content="前言
本安装指南是针对 Ubuntu Server 12.04 进行 Hadoop 1.x 单节点伪分布模式配置的。其它系统或其它版本请酌情调整配置方法。
一、准备工作
我们需要有一台 Ubuntu Server 12.04 64位的虚拟机，如果尚未建立，请参考：【在 VirtualBox 中安装 ">
    
    
    
    
    <link rel="alternative" href="/atom.xml" title="Tao Wang&#39;s Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/bear.ico">
    
    
    <link rel="apple-touch-icon" href="/img/bear.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/bear.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/bear.svg" alt="Tao Wang&#39;s Blog" title="Tao Wang&#39;s Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Tao Wang&#39;s Blog">Tao Wang&#39;s Blog</a></h1>
				<h2 class="blog-motto">无善无恶心之体，有善有恶意之动，知善知恶是良知，为善去恶是格物。</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:twang2218.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/tutorial/hadoop-install/hadoop1-single-node-install-guide.html" title="Hadoop 1.2.1 单节点伪分布配置" itemprop="url">Hadoop 1.2.1 单节点伪分布配置</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://twang2218.github.io" title="Tao Wang">Tao Wang</a>
    </p>
  <p class="article-time">
    <time datetime="2014-02-18T13:00:00.000Z" itemprop="datePublished">2014年2月19日</time>
    更新日期:<time datetime="2014-05-09T18:17:24.000Z" itemprop="dateModified">2014年5月10日</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一、准备工作"><span class="toc-number">2.</span> <span class="toc-text">一、准备工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、安装_Hadoop"><span class="toc-number">3.</span> <span class="toc-text">二、安装 Hadoop</span></a></li><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1_下载_Hadoop_安装文件"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 下载 Hadoop 安装文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2_配置_Hadoop"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 配置 Hadoop</span></a></li><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1_配置_PATH_环境变量"><span class="toc-number">3.2.1.</span> <span class="toc-text"> 环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2_配置_*-site-xml"><span class="toc-number">3.2.2.</span> <span class="toc-text">*-site.xml</code></span></a></li><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#core-site-xml:"><span class="toc-number">3.2.2.1.</span> <span class="toc-text">:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hdfs-site-xml:"><span class="toc-number">3.2.2.2.</span> <span class="toc-text">:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mapred-site-xml:"><span class="toc-number">3.2.2.3.</span> <span class="toc-text">:</span></a></li></ol></ol></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#三、操作伪分布集群"><span class="toc-number">4.</span> <span class="toc-text">三、操作伪分布集群</span></a></li><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1_格式化_NameNode"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 格式化 NameNode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2_启动_Hadoop"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 启动 Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3_运行示例程序"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 运行示例程序</span></a></li><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1_下载"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.3.1 下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2_解压缩"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.3.2 解压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3_将书籍放到_HDFS_云"><span class="toc-number">4.3.3.</span> <span class="toc-text">3.3.3 将书籍放到 HDFS 云</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4_执行_WordCount_示例程序"><span class="toc-number">4.3.4.</span> <span class="toc-text"> 示例程序</span></a></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4_停止_Hadoop"><span class="toc-number">4.4.</span> <span class="toc-text">3.4 停止 Hadoop</span></a></li></ol>
		</div>
		
		<h1 id="前言">前言</h1>
<p>本安装指南是针对 Ubuntu Server 12.04 进行 Hadoop 1.x 单节点伪分布模式配置的。其它系统或其它版本请酌情调整配置方法。</p>
<h1 id="一、准备工作">一、准备工作</h1>
<p>我们需要有一台 Ubuntu Server 12.04 64位的虚拟机，如果尚未建立，请参考：【<a href="/tutorial/hadoop-install/install-ubuntu-server-12.04-on-virtualbox.html">在 VirtualBox 中安装 Ubuntu Server 12.04 64位版</a>】 进行安装。</p>
<p>此外，我们还需要在这台虚拟机中为 Hadoop 的运行环境进行一些准备工作，具体的内容，请参考： 【<a href="/tutorial/hadoop-install/prepare-hadoop-runtime-environment.html">准备 Hadoop 运行环境</a>】。</p>
<h1 id="二、安装_Hadoop">二、安装 Hadoop</h1>
<h2 id="2-1_下载_Hadoop_安装文件">2.1 下载 Hadoop 安装文件</h2>
<p>我们将安装当前 1.x 系列最新的版本 1.2.1，访问官网：<a href="http://hadoop.apache.org" target="_blank">http://hadoop.apache.org</a> </p>
<p><code>Download Hadoop</code> ⇨ <code>releases</code> ⇨ <code>Download</code> ⇨ <code>Download a release now!</code> ⇨ 选择合适的镜像 ⇨ <code>hadoop-1.2.1</code> ⇨ 我们将下载 <code>hadoop-1.2.1-bin.tar.gz</code> 这个文件。</p>
<p>这个文件可以下载到本地，在传到 Ubuntu 上，但是更好的办法是直接在 Ubuntu 中下载。由于我们已经配置好了静态 IP，因此现在可以在主机上使用 SSH 连接 <code>hadoop-master</code>了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>ssh hadoop-master
</pre></td></tr></table></figure>

<p>登录进去后，我们准备 hadoop 的环境以及下载。我们将创建一个 <code>~/hadoop</code> 目录，以后所有的 hadoop 相关的软件都会放到该目录下。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
</pre></td><td class="code"><pre>mkdir ~/hadoop
<span class="built_in">cd</span> ~/hadoop
</pre></td></tr></table></figure>

<blockquote>
<p>这里需要说明的是，许多教程都最终将 hadoop 放到了 <code>/usr/local/hadoop</code> 目录下，这样做会带来很多权限问题。而很多教程，或者最后干脆使用 <code>root</code> 用户操作一切，或者使用 <code>chown</code> 来解决权限冲突，这都是非常不好的，很多时候是因为写这些教程的人对 Linux 不熟悉，把 Windows 的一些习惯带到了 Linux 里来。生产环境的搭建绝不是这么简单粗暴的改变所有权，更不可能是这样子滥用<code>root</code>权限。除了需要将 hadoop 放到符合 Linux 目录结构的位置外，包括配置文件和日志，也都应该指向正确的位置，并且建立适当的用户组，以及对不同的目录使用不同的权限，这将引入很多额外的工作，因此不适合在初学时涉及。对于开发和实验环境的搭建，我们这里的做法非常简单，足以胜任后续的学习、开发。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>wget http://apache.dataguru.cn/hadoop/common/hadoop-<span class="number">1.2</span>.<span class="number">1</span>/hadoop-<span class="number">1.2</span>.<span class="number">1</span>-bin.tar.gz
</pre></td></tr></table></figure>

<p>如果网络环境较差，则需要检查下载的文件是否正确。执行 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>sha1sum hadoop-<span class="number">1.2</span>.<span class="number">1</span>-bin.tar.gz
</pre></td></tr></table></figure>

<p>如果输出和下面的内容一样，则说明下载正确了。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="number">267748</span>f61c9e27c3e1895453863b435458255939  hadoop-<span class="number">1.2</span><span class="number">.1</span>-bin<span class="preprocessor">.tar</span><span class="preprocessor">.gz</span>
</pre></td></tr></table></figure>

<p>如果下载正确，则对压缩文件进行解压缩：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>tar -zxvf hadoop-<span class="number">1.2</span>.<span class="number">1</span>-bin.tar.gz
</pre></td></tr></table></figure>

<p>执行上述命令后，会生成 <code>hadoop-1.2.1</code> 目录，至此，我们可以开始正式的配置了。</p>
<h2 id="2-2_配置_Hadoop">2.2 配置 Hadoop</h2>
<h3 id="2-2-1_配置_PATH_环境变量">2.2.1 配置 <code>PATH</code> 环境变量</h3>
<p>hadoop 的可执行文件在 <code>hadoop-1.2.1/bin</code> 下，我们如果想执行该文件，或者使用完整路径的方式，或者将该路径加入到 <code>PATH</code> 环境变量中，以后我们直接执行文件即可。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>nano ~/.profile
</pre></td></tr></table></figure>

<p>在最后一行添加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="keyword">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HOME</span>/hadoop/hadoop-<span class="number">1.2</span>.<span class="number">1</span>/bin
</pre></td></tr></table></figure>

<p>保存退出。然后应用该配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">source</span> ~/.profile
</pre></td></tr></table></figure>

<blockquote>
<p>这里注意到有的教程提到的是修改 <code>~/.bashrc</code> 文件。在很多情况下这是工作的，但是某些情况下则不能工作，因此<a href="https://help.ubuntu.com/community/EnvironmentVariables#A.2BAH4ALw.profile" target="_blank">Ubuntu 官网关于环境变量的配置</a> 建议放在<code>~/.profile</code>文件中。</p>
</blockquote>
<p>我们可以通过显示 hadoop 的版本来判断之前的配置是否正确。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>hadoop version
</pre></td></tr></table></figure>

<h3 id="2-2-2_配置_*-site-xml">2.2.2 配置 <code>*-site.xml</code></h3>
<p>之前的配置一直是环境配置，现在才是真正的 Hadoop 相关的配置。对于单节点伪分布模式，配置非常简单，我们只需要修改三个 <code>*-site.xml</code> 文件即可。默认情况下，这三个文件都包含一个空的<code>&lt;configuration&gt;</code> ，在这里，我们需要为三个文件的<code>&lt;configuration&gt;</code>中加入对应的配置。</p>
<p>为方便起见，我们将当前目录换到 <code>conf</code> 目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="built_in">cd</span> hadoop-<span class="number">1.2</span>.<span class="number">1</span>/conf
</pre></td></tr></table></figure>

<h4 id="core-site-xml:"><code>core-site.xml</code>:</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>nano core-site.xml
</pre></td></tr></table></figure>

<p>在 <code>&lt;configuration&gt;</code>中加入：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
        <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hadoop-master:9000<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
    <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
        <span class="tag">&lt;<span class="title">value</span>&gt;</span>${user.home}/hadoop/tmp<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
    <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</pre></td></tr></table></figure>

<ul>
<li><code>fs.default.name</code>： 通过这里指定 NameNode 的位置。我们可以在这里手动指定端口号，也可以忽略以使用默认的端口号。</li>
<li><code>hadoop.tmp.dir</code>：Hadoop 临时文件的位置，默认是 <code>/tmp</code>。这里如果不设置 <code>hadoop.tmp.dir</code>，Hadoop 是可以正常运行的。但由于使用的是 <code>/tmp</code> 目录，即内存，这显然不符合大容量存储的意图，而且数据也会随着关机而丢失。因此，我们需要将目录指定到硬盘上的位置。在这里，我们使用了 <code>${user.home}</code> 变量，该变量代表的是用户主目录 <code>$HOME</code>。 <strong>这个目录需要记住，因为后续排障过程中，可能会需要到这个目录中检查 <code>current/VERSION</code> 文件的<code>namespaceID</code>以及<code>storageID</code>等。</strong></li>
</ul>
<h4 id="hdfs-site-xml:"><code>hdfs-site.xml</code>:</h4>
<p>默认的情况下，<code>dfs.replication</code>，也就是数据块默认副本份数，是3份。由于这里是单节点的伪分布模式，只存在一个 DataNode，因此，数据只可能有1份，所以我们需要将其修改为1。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>nano hdfs-site.xml
</pre></td></tr></table></figure>

<p>在 <code>&lt;configuration&gt;</code>中加入：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre>    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
        <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
    <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</pre></td></tr></table></figure>

<h4 id="mapred-site-xml:"><code>mapred-site.xml</code>:</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>nano mapred-site.xml
</pre></td></tr></table></figure>

<p>在 <code>&lt;configuration&gt;</code>中加入：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre>    <span class="tag">&lt;<span class="title">property</span>&gt;</span>
        <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="title">name</span>&gt;</span>
        <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop-master:9001<span class="tag">&lt;/<span class="title">value</span>&gt;</span>
    <span class="tag">&lt;/<span class="title">property</span>&gt;</span>
</pre></td></tr></table></figure>

<p>分别保存退出。</p>
<h1 id="三、操作伪分布集群">三、操作伪分布集群</h1>
<h2 id="3-1_格式化_NameNode">3.1 格式化 NameNode</h2>
<p>在配置完成后，我们需要进行一次 NameNode 的格式化。如果未格式化，NameNode 会因 HDFS 所需的数据结构错误而无法启动。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>hadoop namenode -format
</pre></td></tr></table></figure>

<p>格式化之后，应该会出现与下面相似的输出：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="code"><pre><span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hadoop-master/<span class="number">10.0</span>.<span class="number">1.110</span>
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = <span class="number">1.2</span>.<span class="number">1</span>
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-<span class="number">1.2</span> -r <span class="number">1503152</span>; compiled by <span class="string">'mattf'</span> on Mon Jul <span class="number">22</span> <span class="number">15</span>:<span class="number">23</span>:<span class="number">09</span> PDT <span class="number">2013</span>
STARTUP_MSG:   java = <span class="number">1.7</span>.<span class="number">0</span>_51
************************************************************/
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO util.GSet: Computing capacity <span class="keyword">for</span> map BlocksMap
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO util.GSet: VM <span class="built_in">type</span>       = <span class="number">64</span>-bit
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO util.GSet: <span class="number">2.0</span>% max memory = <span class="number">1013645312</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO util.GSet: capacity      = <span class="number">2</span>^<span class="number">21</span> = <span class="number">2097152</span> entries
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO util.GSet: recommended=<span class="number">2097152</span>, actual=<span class="number">2097152</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">16</span> INFO namenode.FSNamesystem: fsOwner=tao
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSNamesystem: supergroup=supergroup
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSNamesystem: isPermissionEnabled=<span class="literal">true</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSNamesystem: dfs.block.invalidate.limit=<span class="number">100</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSNamesystem: isAccessTokenEnabled=<span class="literal">false</span> accessKeyUpdateInterval=<span class="number">0</span> min(s), accessTokenLifetime=<span class="number">0</span> min(s)
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSEditLog: dfs.namenode.edits.toleration.length = <span class="number">0</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.NameNode: Caching file names occuring more than <span class="number">10</span> times 
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO common.Storage: Image file /home/tao/hadoop/tmp/dfs/name/current/fsimage of size <span class="number">109</span> bytes saved <span class="keyword">in</span> <span class="number">0</span> seconds.
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSEditLog: closing edit log: position=<span class="number">4</span>, editlog=/home/tao/hadoop/tmp/dfs/name/current/edits
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.FSEditLog: close success: truncate to <span class="number">4</span>, editlog=/home/tao/hadoop/tmp/dfs/name/current/edits
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO common.Storage: Storage directory /home/tao/hadoop/tmp/dfs/name has been successfully formatted.
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">19</span>:<span class="number">38</span>:<span class="number">17</span> INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hadoop-master/<span class="number">10.0</span>.<span class="number">1.110</span>
************************************************************/
</pre></td></tr></table></figure>

<p>特别是关注其中是否报告格式化成功：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>Storage <span class="built_in">directory</span> /home/tao/hadoop/tmp/dfs/name has been successfully formatted.
</pre></td></tr></table></figure>

<p>如果没有，则需要根据错误信息进行分析。</p>
<p>至此，Hadoop 单节点伪分布模式就配置完毕了。</p>
<h2 id="3-2_启动_Hadoop">3.2 启动 Hadoop</h2>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>start-all.sh
</pre></td></tr></table></figure>

<p>这条命令将根据之前配置的 <code>*-site.xml</code>，来启动 Hadoop 云（当然，此时，我们这个云是伪分布，就一个节点）。</p>
<p>然后，我们需要确定节点是否正常运行了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>jps
</pre></td></tr></table></figure>

<p>这条命令可以帮助我们查看当前系统运行的 Java 程序进程。如果配置正确，我们将看到类似下面的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="number">2613</span> Jps
<span class="number">2018</span> NameNode
<span class="number">2388</span> JobTracker
<span class="number">2143</span> DataNode
<span class="number">2514</span> TaskTracker
<span class="number">2290</span> SecondaryNameNode
</pre></td></tr></table></figure>

<p>需要注意的是，单节点伪分布正确运行后，应该会有5个组件运行：<code>NameNode</code>、<code>DataNode</code>、<code>SecondaryNameNode</code>、<code>JobTracker</code>、以及<code>TaskTracker</code>。缺少任何一个，都说明配置上有所错误。需要回去检查各项配置。</p>
<blockquote>
<p>在出现故障后向论坛、朋友求助的时候，需要给对方提供你的 <code>conf</code> 目录下的配置文件、以及日志，否则对方无法理解你的故障可能的问题。日志目录：<code>$HOME/hadoop/hadoop-1.2.1/logs</code></p>
</blockquote>
<h2 id="3-3_运行示例程序">3.3 运行示例程序</h2>
<p>就如同我们搭建好其它语言的编译环境后，会编写一个 <code>HelloWorld</code> 程序来测试整个环境是否工作一样，在 Hadoop 中，我们有自己的<code>HelloWorld</code>类的程序，叫做<code>WordCount</code>。</p>
<p><code>ＷordCount</code> 是对纯文本进行单词出现次数统计的，为了使用 <code>WordCount</code>，我们需要先准备一些数据，也就是一些文本文件。这里，我们使用 <a href="http://www.gutenberg.org/" target="_blank">古腾堡计划</a>中的英文图书。</p>
<h3 id="3-3-1_下载">3.3.1 下载</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre>mkdir -p ~/hadoop/data/book/download
<span class="built_in">cd</span> ~/hadoop/data/book/download
wget -r <span class="operator">-l</span> <span class="number">2</span> -H <span class="string">"http://www.gutenberg.org/robot/harvest?filetypes[]=txt&langs[]=en"</span>
</pre></td></tr></table></figure>

<p>这里<code>-l 2</code>是只取 2 级，因此并没有下载很多书，大约有200本英文书籍，解压缩后大约 90MB 左右。，如果觉得数据量不够可以酌情增加一些。根据当前的网速，下载需要一段时间。下载完成后，会出现类似于：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
</pre></td><td class="code"><pre>FINISHED <span class="comment">--2014-02-18 19:44:10--</span>
Total wall clock <span class="built_in">time</span>: <span class="number">7</span>m <span class="number">42</span>s
Downloaded: <span class="number">204</span> <span class="built_in">files</span>, <span class="number">34</span>M <span class="operator">in</span> <span class="number">6</span>m <span class="number">39</span>s (<span class="number">86.0</span> KB/s)
</pre></td></tr></table></figure>

<h3 id="3-3-2_解压缩">3.3.2 解压缩</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre><span class="keyword">for</span> f <span class="keyword">in</span> `find ~/hadoop/data/book/download -name *.zip` ; <span class="keyword">do</span> unzip -j <span class="variable">$f</span> *.txt <span class="operator">-d</span> ~/hadoop/data/book/input; <span class="keyword">done</span>
</pre></td></tr></table></figure>

<h3 id="3-3-3_将书籍放到_HDFS_云">3.3.3 将书籍放到 HDFS 云</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>hadoop fs -put ~/hadoop/data/book/input /data/book/input
</pre></td></tr></table></figure>

<p>上传到云后，我们可以用过命令 <code>hadoop fs -ls /data/book/input</code> 来检查是否都已进入云了。</p>
<h3 id="3-3-4_执行_WordCount_示例程序">3.3.4 执行 <code>WordCount</code> 示例程序</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>hadoop jar ~/hadoop/hadoop-<span class="number">1.2</span>.<span class="number">1</span>/hadoop-examples-<span class="number">1.2</span>.<span class="number">1</span>.jar wordcount /data/book/input /data/book/output
</pre></td></tr></table></figure>

<blockquote>
<p>在执行过程中，可以新开一个 SSH 终端窗口连接到 <code>hadoop-master</code>，然后运行命令 <code>htop</code> 来观察任务执行期间的节点负载情况，包括内存占用率、CPU占用率、最消耗资源的程序等等。这些信息将来可能会作为云性能优化调整的依据。</p>
</blockquote>
<p>执行结束后，会输出下面类似的输出：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre></td><td class="code"><pre><span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">29</span> INFO input<span class="preprocessor">.FileInputFormat</span>: Total input paths to process : <span class="number">201</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">29</span> INFO util<span class="preprocessor">.NativeCodeLoader</span>: Loaded the native-hadoop library
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">29</span> WARN snappy<span class="preprocessor">.LoadSnappy</span>: Snappy native library not loaded
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">30</span> INFO mapred<span class="preprocessor">.JobClient</span>: Running job: job_201402190610_0002
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">31</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">0</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">42</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">1</span>% reduce <span class="number">0</span>%
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">26</span>:<span class="number">45</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">2</span>% reduce <span class="number">0</span>%
...
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">32</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">99</span>% reduce <span class="number">32</span>%
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">34</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">32</span>%
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">40</span> INFO mapred<span class="preprocessor">.JobClient</span>:  map <span class="number">100</span>% reduce <span class="number">100</span>%
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>: Job complete: job_201402190610_0002
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>: Counters: <span class="number">29</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Job Counters 
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched reduce tasks=<span class="number">1</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_MAPS=<span class="number">451625</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all reduces waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total time spent by all maps waiting after reserving slots (ms)=<span class="number">0</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Launched map tasks=<span class="number">201</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Data-local map tasks=<span class="number">201</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SLOTS_MILLIS_REDUCES=<span class="number">228055</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Output Format Counters 
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Written=<span class="number">8206570</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:   FileSystemCounters
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_READ=<span class="number">41640360</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_READ=<span class="number">92553354</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     FILE_BYTES_WRITTEN=<span class="number">90941789</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     HDFS_BYTES_WRITTEN=<span class="number">8206570</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:   File Input Format Counters 
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Bytes Read=<span class="number">92529611</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:   Map-Reduce Framework
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output materialized bytes=<span class="number">37792708</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map input records=<span class="number">1868098</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce shuffle bytes=<span class="number">37792708</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Spilled Records=<span class="number">5323388</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output bytes=<span class="number">151082969</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Total committed heap usage (bytes)=<span class="number">27599167488</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     CPU time spent (ms)=<span class="number">251850</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine input records=<span class="number">15649835</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     SPLIT_RAW_BYTES=<span class="number">23743</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input records=<span class="number">2528771</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce input groups=<span class="number">561390</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Combine output records=<span class="number">2636459</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Physical memory (bytes) snapshot=<span class="number">36463890432</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Reduce output records=<span class="number">561390</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Virtual memory (bytes) snapshot=<span class="number">218530009088</span>
<span class="number">14</span>/<span class="number">02</span>/<span class="number">18</span> <span class="number">20</span>:<span class="number">30</span>:<span class="number">41</span> INFO mapred<span class="preprocessor">.JobClient</span>:     Map output records=<span class="number">15542147</span>
</pre></td></tr></table></figure>

<p>如果有错误，会有异常报错。执行下面的命令确定正常执行完毕：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>hadoop fs -ls /data/book/output/
</pre></td></tr></table></figure>

<p>该命令会列出我们指定的HDFS云中的输出目录内容：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre>1
2
3
4
</pre></td><td class="code"><pre>Found <span class="number">3</span> items
<span class="attribute">-rw</span><span class="attribute">-r</span><span class="subst">--</span>r<span class="subst">--</span>   <span class="number">1</span> tao supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">02</span><span class="subst">-</span><span class="number">18</span> <span class="number">20</span>:<span class="number">30</span> /<span class="built_in">data</span>/book/output/_SUCCESS
drwxr<span class="attribute">-xr</span><span class="attribute">-x</span>   <span class="subst">-</span> tao supergroup          <span class="number">0</span> <span class="number">2014</span><span class="subst">-</span><span class="number">02</span><span class="subst">-</span><span class="number">18</span> <span class="number">20</span>:<span class="number">26</span> /<span class="built_in">data</span>/book/output/_logs
<span class="attribute">-rw</span><span class="attribute">-r</span><span class="subst">--</span>r<span class="subst">--</span>   <span class="number">1</span> tao supergroup    <span class="number">8206570</span> <span class="number">2014</span><span class="subst">-</span><span class="number">02</span><span class="subst">-</span><span class="number">18</span> <span class="number">20</span>:<span class="number">30</span> /<span class="built_in">data</span>/book/output/part<span class="attribute">-r</span><span class="subst">-</span><span class="number">00000</span>
</pre></td></tr></table></figure>

<p>注意其中若存在 <code>_SUCCESS</code> 文件，即说明执行成功了。统计的结果就在 <code>part-r-xxxxx</code> 这类文件里。可以拷贝到本地查看其内容，也可以通过 <code>http://hadoop-master:50070</code> 的 Web 界面来查看其内容。</p>
<h2 id="3-4_停止_Hadoop">3.4 停止 Hadoop</h2>
<p>我们知道了怎么启动 Hadoop 云；知道了怎么运行 Hadoop 程序；接下来我们需要停止 Hadoop 云：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre>1
</pre></td><td class="code"><pre>stop-all.sh
</pre></td></tr></table></figure>

<p>可以通过 <code>jps</code> 确定一下确实都关闭了。如果需要关机，则执行 <code>sudo poweroff</code> 即可。</p>
  
		<p>【该文档最新版本请查看： <a href="http://twang2218.github.io/tutorial/hadoop-install/hadoop1-single-node-install-guide.html">http://twang2218.github.io/tutorial/hadoop-install/hadoop1-single-node-install-guide.html</a> 】</p>

	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/hadoop/">hadoop</a><a href="/tags/hadoop install/">hadoop install</a>
  </div>


<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/tutorial/">tutorial</a>
</div>



<div class="article-share" id="share">

  <div data-url="http://twang2218.github.io/tutorial/hadoop-install/hadoop1-single-node-install-guide.html" data-title="Hadoop 1.2.1 单节点伪分布配置 | Tao Wang&#39;s Blog" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/tutorial/hadoop-install/compile-hadoop2.html" title="编译 Hadoop 2.2.0">
  <strong>PREVIOUS:</strong><br/>
  <span>
  编译 Hadoop 2.2.0</span>
</a>
</div>


<div class="next">
<a href="/tutorial/hadoop-install/prepare-hadoop-runtime-environment.html"  title="准备 Hadoop 运行环境">
 <strong>NEXT:</strong><br/> 
 <span>准备 Hadoop 运行环境
</span>
</a>
</div>

</nav>

	
<section class="comment">
	<div class="ds-thread"></div>
	<div id="disqus_thread"></div>
</section>

</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
			<li><a href="/categories/readings/" title="readings">readings<sup>5</sup></a></li>
		
			<li><a href="/categories/tutorial/" title="tutorial">tutorial<sup>7</sup></a></li>
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			<li><a href="/tags/Hadoop Operations/" title="Hadoop Operations">Hadoop Operations<sup>4</sup></a></li>
		
			<li><a href="/tags/big data/" title="big data">big data<sup>1</sup></a></li>
		
			<li><a href="/tags/hadoop/" title="hadoop">hadoop<sup>10</sup></a></li>
		
			<li><a href="/tags/hadoop install/" title="hadoop install">hadoop install<sup>6</sup></a></li>
		
			<li><a href="/tags/vagrant/" title="vagrant">vagrant<sup>1</sup></a></li>
		
		</ul>
</div>


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font" class="clearfix">
		
		
		
		<a href="https://github.com/twang2218" target="_blank" title="github"></a>
		
		
	</div>
		<p class="copyright">Powered by <a href="http://zespia.tw/hexo/" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2014 
		
		<a href="http://twang2218.github.io" target="_blank" title="Tao Wang">Tao Wang</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>


<script type="text/javascript">
  var duoshuoQuery = {short_name:"twang2218-github"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
    || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script> 



<script type="text/javascript">
  var disqus_shortname = 'twang2218-github';
  
  var disqus_url = 'http://twang2218.github.io/tutorial/hadoop-install/hadoop1-single-node-install-guide.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//go.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>





<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-3066512-10', 'auto');  
ga('send', 'pageview');
</script>


  </body>
</html>
